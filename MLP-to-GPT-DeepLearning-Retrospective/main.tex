%! TeX program = pdflatex
\documentclass[10pt]{beamer}
%% ! TeX program = lualatex

%% Based on the original theme by Matthias Vogelgesang

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\usepackage{graphicx} %LaTeX package to import graphics
\graphicspath{{images/}} %configuring the graphicx package

\usepackage{animate}

\usepackage{caption}
\captionsetup{font=small}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNCC Theme Adjustments %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{CanvasBG}{HTML}{FAFAFA}

% From the official style guide
\definecolor{UnccGreen}{HTML}{005035}
\definecolor{UnccLightGreen}{HTML}{C3D7A4}
\definecolor{UnccGold}{HTML}{A49665}
\definecolor{UnccOrange}{HTML}{F3901D}
\definecolor{UnccLightYellow}{HTML}{899064}
\definecolor{UnccBlue}{HTML}{007377}
\definecolor{UnccPink}{HTML}{DE3A6E}
\definecolor{White}{HTML}{FFFFFF}
\definecolor{LightGray}{HTML}{F1E6B2}

\setbeamercolor{frametitle}{bg=UnccGreen}
\setbeamercolor{progress bar}{bg=UnccGold, fg=UnccGreen}
\setbeamercolor{alerted text}{fg=UnccOrange}

\setbeamercolor{block title}{bg=UnccGreen, fg=White}
\setbeamercolor{block title example}{bg=UnccBlue, fg=White}
\setbeamercolor{block title alerted}{bg=UnccPink, fg=White}
\setbeamercolor{block body}{bg=LightGray}

\metroset{titleformat=smallcaps, progressbar=foot}

\makeatletter
\setlength{\metropolis@progressinheadfoot@linewidth}{2pt}
\setlength{\metropolis@titleseparator@linewidth}{2pt}
\setlength{\metropolis@progressonsectionpage@linewidth}{2pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNCC Theme Adjustments %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{MLP to GPT: A Deep Learning Retrospective}
\subtitle{a brief history of break-throughs in nlp}

\begin{document}

\maketitle

% %=======================================================================================================================
% \begin{frame}[fragile]{What we plan to cover}
%
% 	- Make this the last slide you prepare
%
% 	% - Idea is\\
% 	% Goal (aka why should I care)\\
% 	% - If you are reading a research paper or referencing PyTorch/TF reference (you should be able to connect it to something)\\
% 	% - What's backprop\\
% 	% - linear regression\\
% 	% - MLP\\
% 	% - what's non-linearity\\
% 	% - main mathematical equation\\
% 	% - what are usages\\
% 	% - special cases\\
% 	% - RNN, CNN, GRU, LSTM\\
% 	% - Word Embeddings\\
% 	% - Language Models\\
% 	% - seq-to-seq models (explain the concept of time)\\
% 	% - explain how this create a time boundation (the issue solved by transformer later)\\
% 	% - What's (general or cross) attention?\\
% 	% - What's self attention\\
% 	% - Multi-headed attention\\
% 	%
% 	% - Transformers\\
% 	% - BERT\\
% 	% - GPT\\
% 	% - LLAMA\\
% 	% - MISTRAL\\
% 	% - Gemini / Gemma\\
% 	% - BARD\\
% 	% - Orca\\
% 	% - MPT-7B\\
% 	% - T5
% 	% - DeciLM-7B
% 	%
% 	% % any other relevant state-of-the-art models\\
% 	% - Topics for future / Things to look out for in DL space :\\
% 	% - RAG\\
% 	% % Look up recent research (+ chatGPT, Vishruit, Roshin etc.)
% 	%
% 	% Why should I care:
%
% \end{frame}

%=======================================================================================================================
\begin{frame}{Table of contents}
	\setbeamertemplate{section in toc}[sections numbered]
	\tableofcontents%[hideallsubsections]
\end{frame}

%#######################################################################################################################
\section[Pre-Transformer Developments]{Pre-Transformer Developments}

%=======================================================================================================================
\begin{frame}[fragile]{ Multi-layered Perceptron }

	A \textbf{Multi-Layer Perceptron (MLP)} is a name for a modern feedforward artificial neural network
	\textit{consisting of fully connected neurons with a nonlinear activation function, organized in at least three
		layers}. Modern feedforward networks are trained using the \textbf{backpropagation method} and are
	colloquially referred to as the \textit{\textbf{"vanilla"}} neural networks.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{mlp_main}
		\caption{
			\href{https://playground.tensorflow.org/}{Tensorflow Playground}
		}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Recurrent Neural Networks }

	\textbf{Recurrent Neural Networks (RNNs)} are a class of neural networks designed for processing sequential
	data. The key differentiator with RNNs is the \textit{hidden state} which capture information (context) about
	the sequence so far, enabling them to "remember" previous inputs.
	\begin{equation}
		h_t = \tanh(W_h * h_{t-1} + W_x * x_t + b)
	\end{equation}

	\begin{figure}[h]
		\centering
		% TODO: Update this!
		\animategraphics[autoplay,loop,width=0.60\textwidth]{35}{images/rnn_1/rnn_1-}{0}{1}
		% \animategraphics[autoplay,loop,width=0.75\textwidth]{30}{images/rnn_1/rnn_1-}{0}{408}
		\caption{jalammar.github.io}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Vanishing and Exploding Gradients}

	The vanishing and exploding gradient problems are common issues encountered during the training of recurrent
	neural networks (RNNs) due to large number of steps/layers in the backpropagation process wherein the gradients
	either decrease or increase exponentially in which case they become unusable (vanishing gradients carry too
	little information; exploding gradients are too unstable).

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{vanishing_gradients}
		% \caption{
		% 	\href{https://playground.tensorflow.org/}{Tensorflow Playground}
		% }
	\end{figure}

	% ### Mitigating Strategies:

	% 1. **Weight Initialization:** Using appropriate initialization methods for network weights, such as Xavier or He
	% initialization, can help alleviate both vanishing and exploding gradient problems by ensuring that gradients
	% neither vanish nor explode as they propagate through the network.
	%
	% 2. **Activation Functions:** Choosing activation functions that avoid saturation (e.g., ReLU, Leaky ReLU) can
	% help mitigate the vanishing gradient problem by allowing gradients to propagate more effectively through the
	% network.
	%
	% 3. **Gradient Clipping:** Limiting the magnitude of gradients during training (gradient clipping) can prevent
	% the exploding gradient problem by ensuring that gradients do not become too large.
	%
	% 4. **Normalization Techniques:** Batch normalization and layer normalization can help stabilize training and
	% mitigate both vanishing and exploding gradient problems by normalizing the input to each layer.
	%
	% 5. **Architectural Modifications:** Using architectures like Long Short-Term Memory (LSTM) or Gated Recurrent
	% Unit (GRU) in RNNs can help mitigate the vanishing gradient problem by introducing mechanisms to better capture
	% long-range dependencies.
	%
	% By applying these strategies, practitioners can address the vanishing and exploding gradient problems, allowing
	% for more stable and effective training of deep neural networks.

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Long Short-Term Memory (LSTM)}

	\textit{"Itâ€™s difficult for vanilla RNNs to learn to preserve information over many timesteps. How about an RNN with
		memory?"}

	\textbf{Long Short-Term Memory (LSTM)} is a type of RNN architecture designed to address the traditional RNN's
	difficulty in learning long-term dependencies due to the vanishing gradient problem.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{LSTM}
		\caption{CS224N : NLP with Deep Learning}
	\end{figure}

\end{frame}


%=======================================================================================================================
\begin{frame}[fragile]{Long Short-Term Memory (LSTM)}

	LSTM has following key components:

	\begin{itemize}
		\item Memory Cell: Maintains information over time.
		\item Forget Gate: Decides what information to throw away from cell-state.
		\item Input Gate: Decides which new information to add to the cell-state.
		\item Output Gate: Controls how much of the cell state is exposed to the output.
	\end{itemize}

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{LSTM}
	\end{figure}

\end{frame}

% %=======================================================================================================================
% \begin{frame}[fragile]{Recurrent Neural Networks}
%
% 	\begin{figure}[h]
% 		\centering
% 		\includegraphics[width=0.6\textwidth]{rnn_types}
% 	\end{figure}
%
% \end{frame}

% %=======================================================================================================================
% \begin{frame}[fragile]{Recurrent Neural Networks : Sequence-To-Sequence}
%
% 	RNNs are often used in applications where input and output are in sequence format e.g. machine translation, text
% 	prediction, speech recognition, stock price forecasting etc.
%
% 	\begin{figure}[h]
% 		\centering
% 		\includegraphics[width=0.75\textwidth]{seq2seq_explained}
% 		\caption{Sequence to Sequence Learning with Neural Networks by Ilya Sutskever, et al.}
% 	\end{figure}
%
% \end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Recurrent Neural Networks : Encoders \& Decoders}

	The encoder processes the input sequence and distills it into a fixed-length representation, often referred to
	as the \textbf{context vector}. This vector serves as a condensed summary of the input, capturing its essence
	for the decoder to interpret.

	% Challenges:
	%
	% When faced with lengthy and information-dense input sequences, the modelâ€™s ability to maintain relevance across
	% the entire decoding process can wane. Not all fragments of the inputâ€™s context are equally pertinent at every
	% juncture of text generation. For instance, in machine translation, the word â€˜boyâ€™ in the sentence â€˜A boy is
	% eating the bananaâ€™ does not necessitate the full breadth of the sentenceâ€™s context for accurate translation.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{encoder_decoder}
		\caption{Seq2Seq Learning with Neural Networks by Ilya Sutskever, et al.}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Recurrent Neural Network : Encoders \& Decoders}

	\begin{figure}[h]
		\centering
		% TODO: Update this!
		\animategraphics[autoplay,loop,width=0.75\textwidth]{10}{images/seq2seq_2/seq2seq_2-}{1}{2}
		% \animategraphics[autoplay,loop,width=0.75\textwidth]{40}{images/seq2seq_2/seq2seq_2-}{1}{573}
		% \caption{
		% 	\href{https://playground.tensorflow.org/}{Tensorflow Playground}
		% }
	\end{figure}

	\begin{figure}[h]
		\centering
		% TODO: Update this!
		\animategraphics[autoplay,loop,width=0.75\textwidth]{10}{images/seq2seq_3/seq2seq_3-}{1}{2}
		% \animategraphics[autoplay,loop,width=0.75\textwidth]{35}{images/seq2seq_3/seq2seq_3-}{1}{519}
		\caption{jalammar.github.io}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Recurrent Neural Network : Drawbacks}

	RNNs have two primary drawbacks that have hindered the advancement of models in NLP applications:

	\begin{itemize}

		\item \textbf{Difficulty in Capturing Long-Term Dependencies:} Despite the introduction of LSTMs, which
		      partially mitigated the vanishing gradient problem, RNN-based models still struggle to capture dependencies
		      across lengthy sequences. This limitation is especially pronounced in tasks requiring the model to retain
		      context over extended input sequences, as the entire context must be compressed into a single hidden state
		      passed between encoder and decoder.

		\item \textbf{Limited Parallelism:} RNNs process input sequences sequentially, where each time step depends on
		      its predecessor. This sequential nature restricts parallel processing during both training and inference,
		      unlike feedforward neural networks that can handle input in parallel.

	\end{itemize}

\end{frame}

%#######################################################################################################################
\section[Attention, Self-Attention, and Transformers]{Attention, Self-Attention, and Transformers}

% %=======================================================================================================================
% \begin{frame}[fragile]{Attention}
%
% 	As the bottleneck for RNN models was the \textit{context vector}, a technique called \textbf{"Attention"} was
% 	proposed which allowed model to focus on relevant parts of input sequence as required by the task at hand preserving
% 	the context from beginning to end.
%
% 	\begin{figure}[h]
% 		\centering
% 		% TODO: Update this!
% 		\animategraphics[autoplay,loop,width=0.75\textwidth]{10}{images/seq2seq_4/seq2seq_4-}{1}{2}
% 		% \animategraphics[autoplay,loop,width=0.75\textwidth]{31}{images/seq2seq_4/seq2seq_4-}{1}{494}
% 		\caption{jalammar.github.io}
% 	\end{figure}
%
% \end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Attention}

	As the bottleneck for RNN models was the \textit{context vector}, a technique called \textbf{"Attention"} was
	proposed which allowed model to focus on relevant parts of input sequence as required by the task at hand preserving
	the context from beginning to end.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{attention_calculation}
	\end{figure}

	% \begin{figure}[h]
	% 	\centering
	% 	% TODO: Update this!
	% 	\animategraphics[autoplay,loop,width=0.75\textwidth]{10}{images/attention_process/attention_process-}{1}{2}
	% 	%\animategraphics[autoplay,loop,width=0.75\textwidth]{30}{images/attention_process/attention_process-}{1}{541}
	% 	\caption{jalammar.github.io}
	% \end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Attention}

	\begin{figure}[h]
		\centering
		% TODO: Update this!
		\animategraphics[autoplay,loop,width=\textwidth]{10}{images/attention_tensor_dance/attention_tensor_dance-}{1}{2}
		% \animategraphics[autoplay,loop,width=\textwidth]{30}{images/attention_tensor_dance/attention_tensor_dance-}{1}{931}
		\caption{jalammar.github.io}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Attention}

	As observed in the image below, the model learns from the training phase how to align words in the language pair.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{attention_alignment}
	\end{figure}

\end{frame}



%=======================================================================================================================
\begin{frame}[fragile]{Self-Attention}

	Self Attention, as the name suggests, consists of attention score calculated for an input sequence and then applied
	to itself. Expanding upon the attention compution from previous slide.

	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{attention_calculation_annotated}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Self-Attention}

	Following is the mathematical (matrix) representation of self-attention for input token `\(X\)`. \(W^Q\), \(W^K\), and
	\(W^V\) are weight matrices trained through unsupervised and supervised learning tasks.

	\begin{columns}[c]
		\column{0.4\textwidth}
		\begin{center}
			\includegraphics[width=\textwidth]{self_attention1}
		\end{center}
		\column{0.6\textwidth}
		\begin{center}
			\includegraphics[width=\textwidth]{self_attention2}
		\end{center}
	\end{columns}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Multi-Headed Self-Attention}

	Multi-headed attention is just replication of self-attention multiple times which allow projection of input
	embeddings into different representation subspaces.

	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{multi_head_attention}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Transformers}

	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{transformer_architecture}
	\end{figure}

\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Transformers: Encoder and Decoder}

	\begin{columns}[c]
		\column{0.4\textwidth}
		\begin{itemize}
			\item Encoder's job is to learn the semantics, structure and nitty-gritties of the language
			\item Decoder's job is to do the calculation / computation and give the output for a given task (language
			      modelling, question-answering, summarization etc.)
		\end{itemize}
		\column{0.6\textwidth}
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.75\textwidth]{transformer_architecture2}
		\end{figure}
	\end{columns}

\end{frame}


%#######################################################################################################################
\section[Generative Pre-trained Transformer (GPT)]{Generative Pre-trained Transformer (GPT)}

%=======================================================================================================================
\begin{frame}[fragile]{GPT}

	Enable the theme by loading

\end{frame}

% %#######################################################################################################################
% \section[Quickstart]{quickstart}
%
% %=======================================================================================================================
% \begin{frame}[fragile]{starting a new nlp project}
%
% 	Starting Point for a new project -\\
% 	- Task based split\\
% 	- General Papers\\
%
% \end{frame}
%
% %=======================================================================================================================
% \begin{frame}[fragile]{starting a new DL project}
%
% 	Github Reference Guide
%
% \end{frame}

% %=======================================================================================================================
% \begin{frame}[fragile]{resources}
%
% 	Awesome NLP Papers
%
% \end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Brief Recent History of NLP Deep-Learning Models}
	\begin{itemize}

		\item \textbf{1965:} \textit{First feedforward network} was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa
		      % Q: How was this network trained? 

		\item \textbf{1967:} \textit{Stochastic gradient descent} was used for the first time for training neural network

		\item \textbf{1982:} \textit{BackPropogation} method was applied in the way that has become standard, for the first time by Paul Werbos.

		\item \textbf{1986:} David Rumelhart, Geoffrey Hinton, and Ronald Williams developed the idea of
		      \textit{backpropagation through time (BPTT)} to train RNNs.

		      % This was a significant step forward because it provided a method to train RNNs by unfolding them
		      % in time and applying backpropagation.

		\item \textbf{1997:} Sepp Hochreiter and JÃ¼rgen Schmidhuber introduced \textit{Long Short-Term Memory (LSTM)} networks.

	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Brief Recent History of NLP Deep-Learning Models}
	\begin{itemize}

		\item \textbf{2013:} Mikolov et al. developed \textit{Word2Vec} to produce word embeddings (dense vector
		      representations of words in a continuous vector space).

		      % Word2Vec is a set of neural network models designed to produce word embeddings, which are dense
		      % vector representations of words in a continuous vector space. These embeddings capture semantic
		      % relationships between words based on their context in a large corpus of text.

		\item \textbf{2014:} Ilya Sutskever et al. introduced the \textit{Sequence-to-Sequence (Seq2Seq)} model,
		      a type of RNN architecture which used \textit{encoder-decoder framework} for tasks such as
		      machine translation.

		      % - **Cho et al.** introduced the Gated Recurrent Unit (GRU), a simpler alternative to LSTMs that also addresses the
		      % vanishing gradient problem and improves training efficiency.

		\item \textbf{2015-2016:} \textit{Attention Mechanisms} were introduced to improve Seq2Seq models.

		\item \textbf{2017:} Vaswani et al. published the \textit{"Attention Is All You Need"} paper, introducing
		      the Transformer model.

		\item \textbf{2018:} Radford et al. published the \textit{"Improving Language Understanding by Generative
			      Pre-Training"} paper, introducing the GPT-1 model.

	\end{itemize}
\end{frame}

%=======================================================================================================================
\begin{frame}[fragile]{Resources}

	\begin{itemize}
		\item Jay Alammar Blog (\href{https://jalammar.github.io/}{\color{UnccPink}{\underline{https://jalammar.github.io/}}})
		\item Paper Discussion(s) by Tanay Mehta:
		      \begin{itemize}
			      \item \href{https://www.youtube.com/watch?v=adGf740I0e8}{\color{UnccPink}{\underline{Attention is all you need}}}
			      \item \href{https://www.youtube.com/watch?v=X_haaed9Lfo}{\color{UnccPink}{\underline{Improving
						            Language Understanding by Generative Pre-Training}}}
		      \end{itemize}
		\item
		      \href{https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec4-pretraining.pdf}{\color{UnccPink}{\underline{
					      Princeton COS 598C: Deep Learning for NLP lecture notes}}}
		\item \href{https://www.youtube.com/watch?v=kCc8FmEb1nY}{\color{UnccPink}{\underline{Let's build GPT: from
					      scratch, in code, spelled out}}} by Andrej Karpathy
	\end{itemize}

\end{frame}

% %=======================================================================================================================
% \begin{frame}{References}
% 	Some references to showcase [allowframebreaks] \cite{yang2023harnessing}
% \end{frame}

% %#######################################################################################################################
% \section{Conclusion}

%=======================================================================================================================
{\setbeamercolor{palette primary}{fg=white, bg=UnccGreen}
\begin{frame}[standout]
	\textbf{Questions?}
\end{frame}
}

% %=======================================================================================================================
% \begin{frame}[allowframebreaks]{References}
%
% 	\bibliography{main}
% 	\bibliographystyle{abbrv}
%
% \end{frame}

\end{document}
