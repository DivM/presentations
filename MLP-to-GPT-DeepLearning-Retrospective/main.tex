%! TeX program = lualatex
\documentclass[10pt]{beamer}

%% Based on the original theme by Matthias Vogelgesang

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNCC Theme Adjustments %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{CanvasBG}{HTML}{FAFAFA}

% From the official style guide
\definecolor{UnccGreen}{HTML}{005035}
\definecolor{UnccLightGreen}{HTML}{C3D7A4}
\definecolor{UnccGold}{HTML}{A49665}
\definecolor{UnccOrange}{HTML}{F3901D}
\definecolor{UnccLightYellow}{HTML}{899064}
\definecolor{UnccBlue}{HTML}{007377}
\definecolor{UnccPink}{HTML}{DE3A6E}
\definecolor{White}{HTML}{FFFFFF}
\definecolor{LightGray}{HTML}{F1E6B2}

\setbeamercolor{frametitle}{bg=UnccGreen}
\setbeamercolor{progress bar}{bg=UnccGold, fg=UnccGreen}
\setbeamercolor{alerted text}{fg=UnccOrange}

\setbeamercolor{block title}{bg=UnccGreen, fg=White}
\setbeamercolor{block title example}{bg=UnccBlue, fg=White}
\setbeamercolor{block title alerted}{bg=UnccPink, fg=White}
\setbeamercolor{block body}{bg=LightGray}

\metroset{titleformat=smallcaps, progressbar=foot}

\makeatletter
\setlength{\metropolis@progressinheadfoot@linewidth}{2pt}
\setlength{\metropolis@titleseparator@linewidth}{2pt}
\setlength{\metropolis@progressonsectionpage@linewidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNCC Theme Adjustments %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{MLP to GPT: A Deep Learning Retrospective}
\subtitle{a brief history of break-throughs in nlp}

\begin{document}

\maketitle

\begin{frame}[fragile]{What we plan to cover}

	- Idea is
	Goal (aka why should I care)
	- If you are reading a research paper or referencing PyTorch/TF reference (you should be able to connect it to
	something)
	- What's backprop
	- linear regression
	- MLP
	- what's non-linearity
	- main mathematical equation
	- what are usages
	- special cases
	- RNN, CNN, GRU, LSTM
	- Word Embeddings
	- Language Models
	- seq-to-seq models (explain the concept of time)
	- explain how this create a time boundation (the issue solved by transformer later)
	- What's (general or cross) attention?
	- What's self attention
	- Multi-headed attention
	- Transformers

	Why should I care:

\end{frame}

\begin{frame}{Table of contents}
	\setbeamertemplate{section in toc}[sections numbered]
	\tableofcontents%[hideallsubsections]
\end{frame}

\section[Multi-layered Perceptron]{mlp-section}

\begin{frame}[fragile]{ Multi-layered Perceptron }

	A \textbf{multilayer perceptron (MLP)} is a name for a modern feedforward artificial neural network
	\textit{consisting of fully connected neurons with a nonlinear activation function, organized in at least three
		layers}.


	Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the
	"vanilla" neural networks.

\end{frame}

\begin{frame}[fragile]{ Multi-layered Perceptron }

	Timeline[edit] In 1958, a layered network of perceptrons, consisting of an input layer, a hidden layer with
	randomized weights that did not learn, and an output layer with learning connections, was introduced already by
	Frank Rosenblatt in his book Perceptron.[8][9][10] This extreme learning machine[11][10] was not yet a deep
	learning network. In 1965, the first deep-learning feedforward network, not yet using stochastic gradient
	descent, was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa, at the time called the Group Method
	of Data Handling.[12][13][10]

	In 1967, a deep-learning network, which used stochastic gradient descent for the first time, able to classify
	non-linearily separable pattern classes, was published by Shun'ichi Amari.[14] Amari's student Saito conducted
	the computer experiments, using a five-layered feedforward network with two learning layers. In 1970, modern
	backpropagation method, an efficient application of a chain-rule-based supervised learning,[15][16] was for the
	first time published by the Finnish researcher Seppo Linnainmaa.[2][17][10] The term (i.e. "back-propagating
	errors") itself has been used by Rosenblatt himself,[9] but he did not know how to implement it,[10] although a
	continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J.
	Kelley.[3][10] It is known also as a reverse mode of automatic differentiation.

	In 1982, backpropagation was applied in the way that has become standard, for the first time by Paul
	Werbos.[5][10]

	In 1985, an experimental analysis of the technique was conducted by David E. Rumelhart et al.[6] Many
	improvements to the approach have been made in subsequent decades,.[10]

	In 1990s, an (much simpler) alternative to using neural networks, although still related[18] support vector
	machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear
	classification, they were able to efficiently perform a non-linear classification using what is called the
	kernel trick, using high-dimensional feature spaces.

	In 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to
	language modelling by Yoshua Bengio with co-authors.[19]

	In 2017, modern transformer architectures has been introduced.[20][21]

	In 2021, a very simple NN architecture combining two deep MLPs with skip connections and layer normalizations
	was designed and called MLP-Mixer; its realizations featuring 19 to 431 millions of parameters were shown to be
	comparable to vision transformers of similar size on ImageNet and similar image classification tasks.[22]

	\href{https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.15458&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}{Tensorflow Playground}

\end{frame}

\section[Recurrent Neural Network]{rnn}

\begin{frame}[fragile]{Recurrent Neural Network}

	Enable the theme by loading

\end{frame}

\section[LSTM]{lstm}

\begin{frame}[fragile]{LSTM}

	Enable the theme by loading

\end{frame}



\section[Encoder - Decoder]{encoder-2-decoder}

\begin{frame}[fragile]{Encoder - Decoder Architectures}

	Enable the theme by loading

\end{frame}


\section[Sequence-to-sequence models]{seq-2-seq}

\begin{frame}[fragile]{Sequence To Sequence}


\end{frame}

\section[Attention]{attention}

\begin{frame}[fragile]{Attention}

	Enable the theme by loading

\end{frame}


\section[Self-Attention]{self-attention}

\begin{frame}[fragile]{self-attention}

	Enable the theme by loading

\end{frame}


\section[Transformers]{transformers}

\begin{frame}[fragile]{Transformers}

	Enable the theme by loading

\end{frame}

\section[TransformerExamples]{tExamples}

\subsection[BERT]{bert}

\begin{frame}[fragile]{BERT}

	Enable the theme by loading

\end{frame}

\subsection[GPT]{gpt}

\begin{frame}[fragile]{GPT}

	Enable the theme by loading

\end{frame}

{%
\setbeamertemplate{frame footer}{My custom footer}
\begin{frame}[fragile]{Frame footer}
	\themename defines a custom beamer template to add a text to the footer. It can be set via
	\begin{verbatim}\setbeamertemplate{frame footer}{My custom footer}\end{verbatim}
\end{frame}
}

\begin{frame}{References}
	Some references to showcase [allowframebreaks] \cite{knuth92,ConcreteMath,Simpson,Er01,greenwade93}
\end{frame}

\section{Conclusion}

\begin{frame}{Summary}

	Get the source of this theme and the demo presentation from

	\begin{center}\url{github.com/matze/mtheme}\end{center}

	The theme \emph{itself} is licensed under a
	\href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons
		Attribution-ShareAlike 4.0 International License}.

	\begin{center}\ccbysa\end{center}

\end{frame}

{\setbeamercolor{palette primary}{fg=black, bg=yellow}
\begin{frame}[standout]
	Questions?
\end{frame}
}

\appendix

\begin{frame}[fragile]{Backup slides}
	Sometimes, it is useful to add slides at the end of your presentation to
	refer to during audience questions.

	The best way to do this is to include the \verb|appendixnumberbeamer|
	package in your preamble and call \verb|\appendix| before your backup slides.

	\themename will automatically turn off slide numbering and progress bars for
	slides in the appendix.
\end{frame}

\begin{frame}[allowframebreaks]{References}

	\bibliography{demo}
	\bibliographystyle{abbrv}

\end{frame}

\end{document}
